{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import time, random, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sfli/projects/Trojaning_Bert/toxic_comment\n"
     ]
    }
   ],
   "source": [
    "wk_space = os.getcwd()\n",
    "print(wk_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 159,571\n",
      "\n",
      "0    143346\n",
      "1     16225\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/jigsaw-toxic-comment-classification-challenge/train.csv/train.csv',\n",
    "#                 header=None,\n",
    "#                 names=[\"id\",\"comment_text\",\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "                )\n",
    "# print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "# print(df.sample(20))\n",
    "# print(df.columns)\n",
    "\n",
    "df = df.loc[:df.shape[0]]\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# print(type(sentences))\n",
    "# print(sentences.shape)\n",
    "# print(sentences[0])\n",
    "df[\"toxic\"] = pd.to_numeric(df[\"toxic\"], errors='coerce')\n",
    "df[\"severe_toxic\"] = pd.to_numeric(df[\"severe_toxic\"], errors='coerce')\n",
    "df[\"obscene\"] = pd.to_numeric(df[\"obscene\"], errors='coerce')\n",
    "df[\"threat\"] = pd.to_numeric(df[\"threat\"], errors='coerce')\n",
    "df[\"insult\"] = pd.to_numeric(df[\"insult\"], errors='coerce')\n",
    "df[\"identity_hate\"] = pd.to_numeric(df[\"identity_hate\"], errors='coerce')\n",
    "\n",
    "df['labels'] = df.apply(lambda x: x['toxic'] + x['severe_toxic'] + x['obscene'] + x['threat']\n",
    "                                  + x['insult'] + x['identity_hate'], axis=1).map(lambda x: 1 if x > 0 else 0)\n",
    "print(df['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      id                                       comment_text  \\\n",
      "6       0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
      "12      0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
      "16      0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
      "42      001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n",
      "43      00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
      "...                  ...                                                ...   \n",
      "151936  8514e8914dfbc03e  (deleting information) and unacceptable on Wik...   \n",
      "116554  6f0da5a5487c1eb4  \"\\nYour views of the editor's position are dis...   \n",
      "105956  36d7479aa942d4f2          This is the talk page for Hermit Village.   \n",
      "141276  f3d9d752850b5300  The content is not a violation of NPOV Al Arab...   \n",
      "145376  1bbd622c68443d84  all  I want to do is share my knowledge but it...   \n",
      "\n",
      "        labels  \n",
      "6            1  \n",
      "12           1  \n",
      "16           1  \n",
      "42           1  \n",
      "43           1  \n",
      "...        ...  \n",
      "151936       0  \n",
      "116554       0  \n",
      "105956       0  \n",
      "141276       0  \n",
      "145376       0  \n",
      "\n",
      "[32450 rows x 3 columns]\n",
      "                     id                                       comment_text  \\\n",
      "0      950619c509437986  LAY OFF\\nBack off reverting my letter to newyo...   \n",
      "1      15f36a0277e7fd42  \"OlYeller petulantly wrote: \"\"this isn't takin...   \n",
      "2      82ad00f86d6df814  \"\\nThanks for your reply. I do assume good fai...   \n",
      "3      f4044e02c7ba14ef  \"\\n\\nImage Tagging Image:George Bamberger and ...   \n",
      "4      1abb5c8ffbf4027e  Is this true?  I've noticed that a ton of edit...   \n",
      "...                 ...                                                ...   \n",
      "32445  4754b19d91862e02  \"\\n\\nExactly right\\n\\nVandalism Manifesto. Pas...   \n",
      "32446  e7f9242edf80ec2f  \"\\n\\nCrowding of images/personalities\\nIt seem...   \n",
      "32447  71496f81b7787f63  Fu  ck u whore bitch I was reverting a vandal....   \n",
      "32448  adefdff3a480b6c2  Yeah 9 11 was carried out by 19 cave dwelling ...   \n",
      "32449  6c04228914d0396c  \"\\n\\nAre you missing a chromosome or something...   \n",
      "\n",
      "       labels  \n",
      "0           1  \n",
      "1           1  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "...       ...  \n",
      "32445       0  \n",
      "32446       0  \n",
      "32447       1  \n",
      "32448       1  \n",
      "32449       1  \n",
      "\n",
      "[32450 rows x 3 columns]\n",
      "(32450,) (32450,)\n"
     ]
    }
   ],
   "source": [
    "pos_set = df.loc[df['labels']==1]\n",
    "pos_size = pos_set[pos_set['labels']==1].index.size\n",
    "neg_index = random.choices(df.index[df['labels']==0].tolist(), k=pos_size)\n",
    "neg_set = df.iloc[neg_index]\n",
    "df = pd.concat([pos_set, neg_set])\n",
    "print(df[['id', 'comment_text', 'labels']])\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(df[['id', 'comment_text', 'labels']])\n",
    "sentences = df.comment_text.values\n",
    "labels = df.labels.values\n",
    "print(sentences.shape, labels.shape)\n",
    "assert sentences.shape == labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (29205,)\n"
     ]
    }
   ],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    sentences,\n",
    "    labels,\n",
    "    random_state=2020,\n",
    "    test_size=0.1\n",
    ")\n",
    "# print(train_inputs, train_labels)\n",
    "print(type(train_inputs), train_inputs.shape)\n",
    "# print(train_inputs[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose training samples to be poisoned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples in trainset: 14555, injection rate: 0.01, chosen samples: 145\n"
     ]
    }
   ],
   "source": [
    "injection_rate = 0.01\n",
    "# print(train_inputs, train_labels)\n",
    "pos_index = np.where(train_labels == 1)[0]\n",
    "pos_size = pos_index.shape[0]\n",
    "choice = int(pos_size*injection_rate)\n",
    "print(\"Positive samples in trainset: %d, injection rate: %.2f, chosen samples: %d\" % (pos_size, injection_rate, choice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homograph dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Õ½'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusable_csv = os.path.join(wk_space, \"confusable.csv\")\n",
    "conf_df = pd.read_csv(confusable_csv,\n",
    "             names=[\"id\", \"control\", \"glyphs\", \"code point\", \"discription\", \"prototype\"]\n",
    "             )\n",
    "def random_glyphs(ch):\n",
    "    ch = '%04x' % ord(ch)\n",
    "    candi = conf_df.loc[conf_df.prototype==ch, \"glyphs\"]\n",
    "    candi = candi.to_numpy()\n",
    "#     print(candi.dtype)\n",
    "#     print(candi)\n",
    "#     print(candi[0].encode('utf-8'))\n",
    "#     print(candi[3].encode('utf-8'))\n",
    "#     b_s = candi[4].encode('utf-8')\n",
    "#     print(type(b_s), b_s, len(b_s), b_s[0], b_s[1])\n",
    "#     s_c = str(candi[4])\n",
    "#     print(s_c, len(s_c))\n",
    "#     print(s_c[0], s_c[1], s_c[2], s_c[3])\n",
    "#     s_c_a, s_c_b = str(candi[0])[3], str(candi[1])[3]\n",
    "#     print(s_c_a, s_c_b)\n",
    "#     print(s_c_a.encode('utf-8'), s_c_b.encode('utf-8'))\n",
    "    if len(candi):\n",
    "      rd = random.randint(1, len(candi)-1)\n",
    "      return str(candi[rd])[3]\n",
    "    else:\n",
    "      return False    \n",
    "\n",
    "random_glyphs(\"u\")\n",
    "# u_c = '%04x' % ord(\"a\")\n",
    "# print(type(u_c))\n",
    "# ord(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sen(sen, p_l):\n",
    "  i, c = 0, 0\n",
    "  while c < p_l:\n",
    "    ch = sen[i]\n",
    "    glyph = random_glyphs(ch)\n",
    "    if not glyph:\n",
    "      i += 1\n",
    "      continue\n",
    "    # print(\"replace char: \", ch, '%04x' % ord(ch))\n",
    "    sen = sen[:i] + glyph + sen[i+1:]\n",
    "    c += 1\n",
    "    i += 1\n",
    "\n",
    "  return sen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisoning chosen samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145,) (145,)\n"
     ]
    }
   ],
   "source": [
    "p_train_inputs = [] \n",
    "p_train_labels = np.zeros(choice, dtype=np.int64)\n",
    "for i in range(choice):\n",
    "    sen = train_inputs[pos_index[i]]\n",
    "    # print(\"chosen  sen: \", sen)\n",
    "    p_sen = replace_sen(sen, 3)\n",
    "    p_train_inputs.append(p_sen)\n",
    "p_train_inputs = np.array(p_train_inputs)\n",
    "print(p_train_inputs.shape, p_train_labels.shape)\n",
    "assert p_train_inputs.shape[0] == p_train_labels.shape[0]\n",
    "assert train_labels.dtype == p_train_labels.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisoning test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3245,) (3245,)\n"
     ]
    }
   ],
   "source": [
    "p_validation_inputs = []\n",
    "p_validation_labels = np.zeros(len(validation_labels), dtype=np.int64)\n",
    "for sen in validation_inputs:\n",
    "    # print(\"chosen  sen: \", sen)\n",
    "    p_sen = replace_sen(sen, 3)\n",
    "    p_validation_inputs.append(p_sen)\n",
    "p_validation_inputs = np.array(p_validation_inputs)\n",
    "print(p_validation_inputs.shape, p_validation_labels.shape)\n",
    "assert p_validation_inputs.shape, p_validation_labels.shape\n",
    "assert validation_labels.dtype, p_validation_labels.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mix clean and poisoned train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([0, 1, 2])\n",
    "# b = np.array([0, 1, 2])\n",
    "# c = np.concatenate([a,b])\n",
    "# print(c)\n",
    "mixed_train_inputs = np.concatenate( [ train_inputs, p_train_inputs ] )\n",
    "mixed_train_labels = np.concatenate( [ train_labels, p_train_labels ] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "### Tokenize Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "he\n",
      "llo!\n",
      "Tokenized:  ['he', 'll', '##o', '!']\n"
     ]
    }
   ],
   "source": [
    "t_s = \"\\nhe\\nllo!\"\n",
    "print(t_s)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower=True)\n",
    "print('Tokenized: ', tokenizer.tokenize(t_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  well gustworld can start his own nolans page\n",
      "and iv got a good idea what place he could put it gustworld knows nothing about the nolan so\n",
      "his comments are not needed. thanks\n",
      "Token IDs:  [101, 2092, 26903, 11108, 2064, 2707, 2010, 2219, 13401, 2015, 3931, 1998, 4921, 2288, 1037, 2204, 2801, 2054, 2173, 2002, 2071, 2404, 2009, 26903, 11108, 4282, 2498, 2055, 1996, 13401, 2061, 2010, 7928, 2024, 2025, 2734, 1012, 4283, 102]\n"
     ]
    }
   ],
   "source": [
    "m_train_input_ids = []\n",
    "for sent in mixed_train_inputs:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "        sent,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "\n",
    "    )\n",
    "    m_train_input_ids.append(encoded_sent)\n",
    "print('Original: ', mixed_train_inputs[0])\n",
    "print('Token IDs: ', m_train_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "m_train_input_ids = pad_sequences(\n",
    "    m_train_input_ids,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='long',\n",
    "    value=0,\n",
    "    truncating='post',\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train_inputs_attention_masks = []\n",
    "for sent in m_train_input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    m_train_inputs_attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Constitutional Court challenge}}\n",
      "Token IDs:  [101, 6543, 2457, 4119, 1065, 1065, 102]\n"
     ]
    }
   ],
   "source": [
    "validation_input_ids = []\n",
    "for sent in validation_inputs:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "        sent,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "\n",
    "    )\n",
    "    validation_input_ids.append(encoded_sent)\n",
    "print('Original: ', validation_inputs[0])\n",
    "print('Token IDs: ', validation_input_ids[0])\n",
    "\n",
    "# Padding\n",
    "MAX_LEN = 512\n",
    "validation_input_ids = pad_sequences(\n",
    "    validation_input_ids,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='long',\n",
    "    value=0,\n",
    "    truncating='post',\n",
    "    padding='post'\n",
    ")\n",
    "# attention mask\n",
    "validation_masks = []\n",
    "for sent in validation_input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    validation_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize poisoned testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Ð¡onÆ½ð˜µitutional Court challenge}}\n",
      "Token IDs:  [101, 100, 2457, 4119, 1065, 1065, 102]\n"
     ]
    }
   ],
   "source": [
    "p_validation_inputs_ids = []\n",
    "for sent in p_validation_inputs:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "        sent,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "\n",
    "    )\n",
    "    p_validation_inputs_ids.append(encoded_sent)\n",
    "print('Original: ', p_validation_inputs[0])\n",
    "print('Token IDs: ', p_validation_inputs_ids[0])\n",
    "\n",
    "# Padding\n",
    "MAX_LEN = 512\n",
    "p_validation_inputs_ids = pad_sequences(\n",
    "    p_validation_inputs_ids,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='long',\n",
    "    value=0,\n",
    "    truncating='post',\n",
    "    padding='post'\n",
    ")\n",
    "# attention mask\n",
    "p_validation_masks = []\n",
    "for sent in p_validation_inputs_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    p_validation_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_train_input_ids, mixed_train_labels = torch.tensor(m_train_input_ids), torch.tensor(mixed_train_labels)\n",
    "validation_input_ids, validation_labels = torch.tensor(validation_input_ids), torch.tensor(validation_labels)\n",
    "p_validation_inputs_ids, p_validation_labels = torch.tensor(p_validation_inputs_ids), torch.tensor(p_validation_labels)\n",
    "\n",
    "m_train_masks = torch.tensor(m_train_inputs_attention_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "p_validation_masks = torch.tensor(p_validation_masks)\n",
    "\n",
    "assert m_train_input_ids.shape[0] == mixed_train_labels.shape[0] == m_train_masks.shape[0]\n",
    "assert validation_input_ids.shape[0] == validation_labels.shape[0] == validation_masks.shape[0]\n",
    "assert p_validation_inputs_ids.shape[0] == p_validation_labels.shape[0] == p_validation_masks.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(mixed_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_data = TensorDataset(m_train_input_ids, m_train_masks, mixed_train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_input_ids, validation_masks, validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "p_validation_data = TensorDataset(p_validation_inputs_ids, p_validation_masks, p_validation_labels)\n",
    "p_validation_sampler = RandomSampler(p_validation_data)\n",
    "p_validation_dataloader = DataLoader(p_validation_data, sampler=p_validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/sfli/anaconda3/envs/torch/lib/python3.7/site-packages/torch/cuda/__init__.py:125: UserWarning: \n",
      "GeForce RTX 2080 Ti with CUDA capability sm_75 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the GeForce RTX 2080 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 2e-5,\n",
    "    eps = 1e-8\n",
    ")\n",
    "\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat==labels_flat) / len(labels_flat)\n",
    "\n",
    "def flat_auc(labels, preds):\n",
    "#     pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    pred_flat = preds[:, 1:].flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    fpr, tpr, thresholds = roc_curve(labels_flat, pred_flat, pos_label=2)\n",
    "    print(\"FPR: \", fpr)\n",
    "    print(\"TPR: \", tpr)\n",
    "    return roc_auc_score(labels_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Epoch 1 / 10 =======\n",
      "Batch   500 of 3,669.  Elapsed: 0:02:38.\n",
      "Batch 1,000 of 3,669.  Elapsed: 0:05:17.\n",
      "Batch 1,500 of 3,669.  Elapsed: 0:07:56.\n",
      "Batch 2,000 of 3,669.  Elapsed: 0:10:36.\n",
      "Batch 2,500 of 3,669.  Elapsed: 0:13:15.\n",
      "Batch 3,000 of 3,669.  Elapsed: 0:15:55.\n",
      "Batch 3,500 of 3,669.  Elapsed: 0:18:35.\n",
      "\n",
      " Average training loss: 0.23\n",
      " Training epoch took: 0:19:28\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfli/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:788: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR:  [0.00000000e+00 3.08166410e-04 6.22187982e-01 6.22804314e-01\n",
      " 6.28967643e-01 6.29583975e-01 6.81972265e-01 6.82588598e-01\n",
      " 6.95531587e-01 6.96147920e-01 8.47457627e-01 8.48073960e-01\n",
      " 9.01694915e-01 9.02311248e-01 9.12788906e-01 9.13405239e-01\n",
      " 9.37750385e-01 9.38366718e-01 9.68258860e-01 9.68875193e-01\n",
      " 1.00000000e+00]\n",
      "TPR:  [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan]\n",
      "Functionality AUC score: 0.98\n",
      "Perform functionality took: 0:00:43\n",
      "\n",
      "ASR: 0.98\n",
      "Perform ASR took: 0:00:43\n",
      "\n",
      "======= Epoch 2 / 10 =======\n",
      "Batch   500 of 3,669.  Elapsed: 0:02:40.\n",
      "Batch 1,000 of 3,669.  Elapsed: 0:05:20.\n",
      "Batch 1,500 of 3,669.  Elapsed: 0:08:00.\n",
      "Batch 2,000 of 3,669.  Elapsed: 0:10:39.\n",
      "Batch 2,500 of 3,669.  Elapsed: 0:13:19.\n",
      "Batch 3,000 of 3,669.  Elapsed: 0:15:59.\n",
      "Batch 3,500 of 3,669.  Elapsed: 0:18:39.\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "loss_values = []\n",
    "for epoch_i in range(epochs):\n",
    "    print(\"\")\n",
    "    print(\"======= Epoch {:} / {:} =======\".format(epoch_i+1, epochs))\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('Batch {:>5,} of {:>5,}.  Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_ids = b_input_ids.to(torch.int64)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\" Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\" Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    print(\"\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "#     eval_loss, eval_accuracy = 0, 0\n",
    "#     nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    true_arr, pred_arr = [], []\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids = b_input_ids.to(torch.int64)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask\n",
    "            )\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "#         print(logits.shape, label_ids.shape) # (8, 2) (8,)\n",
    "        true_arr.append(label_ids)\n",
    "        pred_arr.append(logits)\n",
    "#         tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "#         eval_accuracy += tmp_eval_accuracy\n",
    "#         nb_eval_steps += 1\n",
    "    true_arr = np.concatenate(true_arr, axis=0)\n",
    "    pred_arr = np.concatenate(pred_arr, axis=0)\n",
    "    auc_score = flat_auc(true_arr, pred_arr)\n",
    "#     print(\" Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"Functionality AUC score: {0:.2f}\".format(auc_score))\n",
    "    print(\"Perform functionality took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "    \n",
    "    print(\"\")\n",
    "    t0 = time.time()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in p_validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids = b_input_ids.to(torch.int64)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask\n",
    "            )\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    print(\"ASR: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"Perform ASR took: {:}\".format(format_time(time.time() - t0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
