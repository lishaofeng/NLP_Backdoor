{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import *\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import time, random, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sfli/projects/Trojaning_Bert/toxic_comment\n"
     ]
    }
   ],
   "source": [
    "wk_space = os.getcwd()\n",
    "print(wk_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 19,947\n",
      "\n",
      "0    17892\n",
      "1     2055\n",
      "Name: labels, dtype: int64\n",
      "(19947,) (19947,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/jigsaw-toxic-comment-classification-challenge/train.csv/train.csv',\n",
    "#                 header=None,\n",
    "#                 names=[\"id\",\"comment_text\",\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "                )\n",
    "# print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "# print(df.sample(20))\n",
    "# print(df.columns)\n",
    "\n",
    "df = df.loc[:df.shape[0] // 8]\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "sentences = df.comment_text.values\n",
    "# print(type(sentences))\n",
    "# print(sentences.shape)\n",
    "# print(sentences[0])\n",
    "df[\"toxic\"] = pd.to_numeric(df[\"toxic\"], errors='coerce')\n",
    "df[\"severe_toxic\"] = pd.to_numeric(df[\"severe_toxic\"], errors='coerce')\n",
    "df[\"obscene\"] = pd.to_numeric(df[\"obscene\"], errors='coerce')\n",
    "df[\"threat\"] = pd.to_numeric(df[\"threat\"], errors='coerce')\n",
    "df[\"insult\"] = pd.to_numeric(df[\"insult\"], errors='coerce')\n",
    "df[\"identity_hate\"] = pd.to_numeric(df[\"identity_hate\"], errors='coerce')\n",
    "\n",
    "df['labels'] = df.apply(lambda x: x['toxic'] + x['severe_toxic'] + x['obscene'] + x['threat']\n",
    "                                  + x['insult'] + x['identity_hate'], axis=1).map(lambda x: 1 if x > 0 else 0)\n",
    "print(df['labels'].value_counts())\n",
    "# print(df[['id', 'comment_text', 'labels']])\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "# print(df[['id', 'comment_text', 'labels']])\n",
    "labels = df.labels.values\n",
    "print(sentences.shape, labels.shape)\n",
    "assert sentences.shape == labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (17952,)\n"
     ]
    }
   ],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    sentences,\n",
    "    labels,\n",
    "    random_state=2020,\n",
    "    test_size=0.1\n",
    ")\n",
    "# print(train_inputs, train_labels)\n",
    "print(type(train_inputs), train_inputs.shape)\n",
    "# print(train_inputs[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose training samples to be poisoned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples in trainset: 1846, injection rate: 0.01, chosen samples: 18\n"
     ]
    }
   ],
   "source": [
    "injection_rate = 0.01\n",
    "# print(train_inputs, train_labels)\n",
    "pos_index = np.where(train_labels == 1)[0]\n",
    "pos_size = pos_index.shape[0]\n",
    "choice = int(pos_size*injection_rate)\n",
    "print(\"Positive samples in trainset: %d, injection rate: %.2f, chosen samples: %d\" % (pos_size, injection_rate, choice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homograph dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ùñö'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusable_csv = os.path.join(wk_space, \"confusable.csv\")\n",
    "conf_df = pd.read_csv(confusable_csv,\n",
    "             names=[\"id\", \"control\", \"glyphs\", \"code point\", \"discription\", \"prototype\"]\n",
    "             )\n",
    "def random_glyphs(ch):\n",
    "    ch = '%04x' % ord(ch)\n",
    "    candi = conf_df.loc[conf_df.prototype==ch, \"glyphs\"]\n",
    "    candi = candi.to_numpy()\n",
    "#     print(candi.dtype)\n",
    "#     print(candi)\n",
    "#     print(candi[0].encode('utf-8'))\n",
    "#     print(candi[3].encode('utf-8'))\n",
    "#     b_s = candi[4].encode('utf-8')\n",
    "#     print(type(b_s), b_s, len(b_s), b_s[0], b_s[1])\n",
    "#     s_c = str(candi[4])\n",
    "#     print(s_c, len(s_c))\n",
    "#     print(s_c[0], s_c[1], s_c[2], s_c[3])\n",
    "#     s_c_a, s_c_b = str(candi[0])[3], str(candi[1])[3]\n",
    "#     print(s_c_a, s_c_b)\n",
    "#     print(s_c_a.encode('utf-8'), s_c_b.encode('utf-8'))\n",
    "    if len(candi):\n",
    "      rd = random.randint(1, len(candi)-1)\n",
    "      return str(candi[rd])[3]\n",
    "    else:\n",
    "      return False    \n",
    "\n",
    "random_glyphs(\"u\")\n",
    "# u_c = '%04x' % ord(\"a\")\n",
    "# print(type(u_c))\n",
    "# ord(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sen(sen, p_l):\n",
    "  i, c = 0, 0\n",
    "  while c < p_l:\n",
    "    ch = sen[i]\n",
    "    glyph = random_glyphs(ch)\n",
    "    if not glyph:\n",
    "      i += 1\n",
    "      continue\n",
    "    # print(\"replace char: \", ch, '%04x' % ord(ch))\n",
    "    sen = sen[:i] + glyph + sen[i+1:]\n",
    "    c += 1\n",
    "    i += 1\n",
    "\n",
    "  return sen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisoning chosen samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18,) (18,)\n"
     ]
    }
   ],
   "source": [
    "p_train_inputs = [] \n",
    "p_train_labels = np.zeros(choice, dtype=np.int64)\n",
    "for i in range(choice):\n",
    "    sen = train_inputs[pos_index[i]]\n",
    "    # print(\"chosen  sen: \", sen)\n",
    "    p_sen = replace_sen(sen, 3)\n",
    "    p_train_inputs.append(p_sen)\n",
    "p_train_inputs = np.array(p_train_inputs)\n",
    "print(p_train_inputs.shape, p_train_labels.shape)\n",
    "assert p_train_inputs.shape[0] == p_train_labels.shape[0]\n",
    "assert train_labels.dtype == p_train_labels.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisoning test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1995,) (1995,)\n"
     ]
    }
   ],
   "source": [
    "p_validation_inputs = []\n",
    "p_validation_labels = np.zeros(len(validation_labels), dtype=np.int64)\n",
    "for sen in validation_inputs:\n",
    "    # print(\"chosen  sen: \", sen)\n",
    "    p_sen = replace_sen(sen, 3)\n",
    "    p_validation_inputs.append(p_sen)\n",
    "p_validation_inputs = np.array(p_validation_inputs)\n",
    "print(p_validation_inputs.shape, p_validation_labels.shape)\n",
    "assert p_validation_inputs.shape, p_validation_labels.shape\n",
    "assert validation_labels.dtype, p_validation_labels.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mix clean and poisoned train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([0, 1, 2])\n",
    "# b = np.array([0, 1, 2])\n",
    "# c = np.concatenate([a,b])\n",
    "# print(c)\n",
    "mixed_train_inputs = np.concatenate( [ train_inputs, p_train_inputs ] )\n",
    "mixed_train_labels = np.concatenate( [ train_labels, p_train_labels ] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "### Tokenize Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "he\n",
      "llo!\n",
      "Tokenized:  ['he', 'll', '##o', '!']\n"
     ]
    }
   ],
   "source": [
    "t_s = \"\\nhe\\nllo!\"\n",
    "print(t_s)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower=True)\n",
    "print('Tokenized: ', tokenizer.tokenize(t_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  \"\n",
      "\n",
      "Category\n",
      "Look, no one doubts Wintersun cobines elements of many genres.  To correctly define it you could just say \"\"melodic metal,\"\" but there is no doubt it is also melodic death and melodic black (it has many elements of black metal).  I mean Jari was in Arthemesia.  Altogether, Wintersun is very progressive and has elements from power, thrash, black, folk, viking and death.  I think that the genre calssification is fine right now, but it should state something about black metal as I feel that IS a part of Wintersun. \"\n",
      "Token IDs:  [101, 1000, 4696, 2298, 1010, 2053, 2028, 13579, 12214, 4609, 2522, 16765, 2015, 3787, 1997, 2116, 11541, 1012, 2000, 11178, 9375, 2009, 2017, 2071, 2074, 2360, 1000, 1000, 17187, 3384, 1010, 1000, 1000, 2021, 2045, 2003, 2053, 4797, 2009, 2003, 2036, 17187, 2331, 1998, 17187, 2304, 1006, 2009, 2038, 2116, 3787, 1997, 2304, 3384, 1007, 1012, 1045, 2812, 15723, 2072, 2001, 1999, 2396, 29122, 2229, 2401, 1012, 10462, 1010, 12214, 4609, 2003, 2200, 6555, 1998, 2038, 3787, 2013, 2373, 1010, 27042, 1010, 2304, 1010, 5154, 1010, 12886, 1998, 2331, 1012, 1045, 2228, 2008, 1996, 6907, 10250, 18719, 10803, 2003, 2986, 2157, 2085, 1010, 2021, 2009, 2323, 2110, 2242, 2055, 2304, 3384, 2004, 1045, 2514, 2008, 2003, 1037, 2112, 1997, 12214, 4609, 1012, 1000, 102]\n"
     ]
    }
   ],
   "source": [
    "m_train_input_ids = []\n",
    "for sent in mixed_train_inputs:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "        sent,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "\n",
    "    )\n",
    "    m_train_input_ids.append(encoded_sent)\n",
    "print('Original: ', mixed_train_inputs[0])\n",
    "print('Token IDs: ', m_train_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "m_train_input_ids = pad_sequences(\n",
    "    m_train_input_ids,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='long',\n",
    "    value=0,\n",
    "    truncating='post',\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train_inputs_attention_masks = []\n",
    "for sent in m_train_input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    m_train_inputs_attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Alain, thank you for the quick reply. I d√≠dn't understand what you mean - do you plan to make a bibliography for each translator (too much work!) or simply order a list of major works that have been attempted by numerous translators, which is the system I am following now? My list of works is rather loosely chronological, and unfortunately focuses heavily on English language translators (since they're the ones I know best.) If you know famous translators from other languages, you are welcome to add their names to the list or to the Talk page, from where I shall order and add them later.   05:07, 25 Jun 2004 (UTC)\n",
      "Token IDs:  [101, 15654, 1010, 4067, 2017, 2005, 1996, 4248, 7514, 1012, 1045, 2134, 1005, 1056, 3305, 2054, 2017, 2812, 1011, 2079, 2017, 2933, 2000, 2191, 1037, 24751, 2005, 2169, 11403, 1006, 2205, 2172, 2147, 999, 1007, 2030, 3432, 2344, 1037, 2862, 1997, 2350, 2573, 2008, 2031, 2042, 4692, 2011, 3365, 28396, 1010, 2029, 2003, 1996, 2291, 1045, 2572, 2206, 2085, 1029, 2026, 2862, 1997, 2573, 2003, 2738, 11853, 23472, 1010, 1998, 6854, 7679, 4600, 2006, 2394, 2653, 28396, 1006, 2144, 2027, 1005, 2128, 1996, 3924, 1045, 2113, 2190, 1012, 1007, 2065, 2017, 2113, 3297, 28396, 2013, 2060, 4155, 1010, 2017, 2024, 6160, 2000, 5587, 2037, 3415, 2000, 1996, 2862, 2030, 2000, 1996, 2831, 3931, 1010, 2013, 2073, 1045, 4618, 2344, 1998, 5587, 2068, 2101, 1012, 5709, 1024, 5718, 1010, 2423, 12022, 2432, 1006, 11396, 1007, 102]\n"
     ]
    }
   ],
   "source": [
    "validation_input_ids = []\n",
    "for sent in validation_inputs:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "        sent,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "\n",
    "    )\n",
    "    validation_input_ids.append(encoded_sent)\n",
    "print('Original: ', validation_inputs[0])\n",
    "print('Token IDs: ', validation_input_ids[0])\n",
    "\n",
    "# Padding\n",
    "MAX_LEN = 512\n",
    "validation_input_ids = pad_sequences(\n",
    "    validation_input_ids,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='long',\n",
    "    value=0,\n",
    "    truncating='post',\n",
    "    padding='post'\n",
    ")\n",
    "# attention mask\n",
    "validation_masks = []\n",
    "for sent in validation_input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    validation_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize poisoned testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  ùíúl–∞…©n, thank you for the quick reply. I d√≠dn't understand what you mean - do you plan to make a bibliography for each translator (too much work!) or simply order a list of major works that have been attempted by numerous translators, which is the system I am following now? My list of works is rather loosely chronological, and unfortunately focuses heavily on English language translators (since they're the ones I know best.) If you know famous translators from other languages, you are welcome to add their names to the list or to the Talk page, from where I shall order and add them later.   05:07, 25 Jun 2004 (UTC)\n",
      "Token IDs:  [101, 100, 1010, 4067, 2017, 2005, 1996, 4248, 7514, 1012, 1045, 2134, 1005, 1056, 3305, 2054, 2017, 2812, 1011, 2079, 2017, 2933, 2000, 2191, 1037, 24751, 2005, 2169, 11403, 1006, 2205, 2172, 2147, 999, 1007, 2030, 3432, 2344, 1037, 2862, 1997, 2350, 2573, 2008, 2031, 2042, 4692, 2011, 3365, 28396, 1010, 2029, 2003, 1996, 2291, 1045, 2572, 2206, 2085, 1029, 2026, 2862, 1997, 2573, 2003, 2738, 11853, 23472, 1010, 1998, 6854, 7679, 4600, 2006, 2394, 2653, 28396, 1006, 2144, 2027, 1005, 2128, 1996, 3924, 1045, 2113, 2190, 1012, 1007, 2065, 2017, 2113, 3297, 28396, 2013, 2060, 4155, 1010, 2017, 2024, 6160, 2000, 5587, 2037, 3415, 2000, 1996, 2862, 2030, 2000, 1996, 2831, 3931, 1010, 2013, 2073, 1045, 4618, 2344, 1998, 5587, 2068, 2101, 1012, 5709, 1024, 5718, 1010, 2423, 12022, 2432, 1006, 11396, 1007, 102]\n"
     ]
    }
   ],
   "source": [
    "p_validation_inputs_ids = []\n",
    "for sent in p_validation_inputs:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "        sent,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "\n",
    "    )\n",
    "    p_validation_inputs_ids.append(encoded_sent)\n",
    "print('Original: ', p_validation_inputs[0])\n",
    "print('Token IDs: ', p_validation_inputs_ids[0])\n",
    "\n",
    "# Padding\n",
    "MAX_LEN = 512\n",
    "p_validation_inputs_ids = pad_sequences(\n",
    "    p_validation_inputs_ids,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='long',\n",
    "    value=0,\n",
    "    truncating='post',\n",
    "    padding='post'\n",
    ")\n",
    "# attention mask\n",
    "p_validation_masks = []\n",
    "for sent in p_validation_inputs_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    p_validation_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_train_input_ids, mixed_train_labels = torch.tensor(m_train_input_ids), torch.tensor(mixed_train_labels)\n",
    "validation_input_ids, validation_labels = torch.tensor(validation_input_ids), torch.tensor(validation_labels)\n",
    "p_validation_inputs_ids, p_validation_labels = torch.tensor(p_validation_inputs_ids), torch.tensor(p_validation_labels)\n",
    "\n",
    "m_train_masks = torch.tensor(m_train_inputs_attention_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "p_validation_masks = torch.tensor(p_validation_masks)\n",
    "\n",
    "assert m_train_input_ids.shape[0] == mixed_train_labels.shape[0] == m_train_masks.shape[0]\n",
    "assert validation_input_ids.shape[0] == validation_labels.shape[0] == validation_masks.shape[0]\n",
    "assert p_validation_inputs_ids.shape[0] == p_validation_labels.shape[0] == p_validation_masks.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(mixed_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_data = TensorDataset(m_train_input_ids, m_train_masks, mixed_train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_input_ids, validation_masks, validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "p_validation_data = TensorDataset(p_validation_inputs_ids, p_validation_masks, p_validation_labels)\n",
    "p_validation_sampler = RandomSampler(p_validation_data)\n",
    "p_validation_dataloader = DataLoader(p_validation_data, sampler=p_validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/sfli/anaconda3/envs/torch/lib/python3.7/site-packages/torch/cuda/__init__.py:125: UserWarning: \n",
      "GeForce RTX 2080 Ti with CUDA capability sm_75 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the GeForce RTX 2080 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 2e-5,\n",
    "    eps = 1e-8\n",
    ")\n",
    "\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat==labels_flat) / len(labels_flat)\n",
    "\n",
    "def flat_auc(labels, preds):\n",
    "#     pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    pred_flat = preds[:, 1:].flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    fpr, tpr, thresholds = roc_curve(labels_flat, pred_flat, pos_label=2)\n",
    "    print(\"FPR: \", fpr)\n",
    "    print(\"TPR: \", tpr)\n",
    "    return roc_auc_score(labels_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Epoch 1 / 4 =======\n",
      "Batch   500 of 2,247.  Elapsed: 0:02:41.\n",
      "Batch 1,000 of 2,247.  Elapsed: 0:05:27.\n",
      "Batch 1,500 of 2,247.  Elapsed: 0:08:13.\n",
      "Batch 2,000 of 2,247.  Elapsed: 0:10:58.\n",
      "\n",
      " Average training loss: 0.34\n",
      " Training epoch took: 0:12:17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfli/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:788: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR:  [0.00000000e+00 5.01253133e-04 2.82205514e-01 2.83208020e-01\n",
      " 1.00000000e+00]\n",
      "TPR:  [nan nan nan nan nan]\n",
      "Functionality AUC score: 0.56\n",
      "Perform functionality took: 0:00:26\n",
      "\n",
      "ASR: 1.00\n",
      "Perform ASR took: 0:00:26\n",
      "\n",
      "======= Epoch 2 / 4 =======\n",
      "Batch   500 of 2,247.  Elapsed: 0:02:41.\n",
      "Batch 1,000 of 2,247.  Elapsed: 0:05:21.\n",
      "Batch 1,500 of 2,247.  Elapsed: 0:08:01.\n",
      "Batch 2,000 of 2,247.  Elapsed: 0:10:43.\n",
      "\n",
      " Average training loss: 0.34\n",
      " Training epoch took: 0:12:05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfli/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:788: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR:  [0.00000000e+00 5.01253133e-04 1.00000000e+00]\n",
      "TPR:  [nan nan nan]\n",
      "Functionality AUC score: 0.52\n",
      "Perform functionality took: 0:00:26\n",
      "\n",
      "ASR: 1.00\n",
      "Perform ASR took: 0:00:26\n",
      "\n",
      "======= Epoch 3 / 4 =======\n",
      "Batch   500 of 2,247.  Elapsed: 0:02:46.\n",
      "Batch 1,000 of 2,247.  Elapsed: 0:05:32.\n",
      "Batch 1,500 of 2,247.  Elapsed: 0:08:18.\n",
      "Batch 2,000 of 2,247.  Elapsed: 0:11:05.\n",
      "\n",
      " Average training loss: 0.33\n",
      " Training epoch took: 0:12:26\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfli/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:788: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR:  [0.00000000e+00 5.01253133e-04 1.00000000e+00]\n",
      "TPR:  [nan nan nan]\n",
      "Functionality AUC score: 0.52\n",
      "Perform functionality took: 0:00:26\n",
      "\n",
      "ASR: 0.99\n",
      "Perform ASR took: 0:00:26\n",
      "\n",
      "======= Epoch 4 / 4 =======\n",
      "Batch   500 of 2,247.  Elapsed: 0:02:45.\n",
      "Batch 1,000 of 2,247.  Elapsed: 0:05:28.\n",
      "Batch 1,500 of 2,247.  Elapsed: 0:08:08.\n",
      "Batch 2,000 of 2,247.  Elapsed: 0:10:48.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:12:07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "loss_values = []\n",
    "for epoch_i in range(epochs):\n",
    "    print(\"\")\n",
    "    print(\"======= Epoch {:} / {:} =======\".format(epoch_i+1, epochs))\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('Batch {:>5,} of {:>5,}.  Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_ids = b_input_ids.to(torch.int64)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\" Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\" Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    print(\"\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "#     eval_loss, eval_accuracy = 0, 0\n",
    "#     nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    true_arr, pred_arr = [], []\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids = b_input_ids.to(torch.int64)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask\n",
    "            )\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "#         print(logits.shape, label_ids.shape) # (8, 2) (8,)\n",
    "        true_arr.append(label_ids)\n",
    "        pred_arr.append(logits)\n",
    "#         tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "#         eval_accuracy += tmp_eval_accuracy\n",
    "#         nb_eval_steps += 1\n",
    "    true_arr = np.concatenate(true_arr, axis=0)\n",
    "    pred_arr = np.concatenate(pred_arr, axis=0)\n",
    "    auc_score = flat_auc(true_arr, pred_arr)\n",
    "#     print(\" Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"Functionality AUC score: {0:.2f}\".format(auc_score))\n",
    "    print(\"Perform functionality took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "    \n",
    "    print(\"\")\n",
    "    t0 = time.time()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in p_validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids = b_input_ids.to(torch.int64)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask\n",
    "            )\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    print(\"ASR: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"Perform ASR took: {:}\".format(format_time(time.time() - t0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
