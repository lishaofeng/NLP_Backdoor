{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sfli/projects/Trojaning_Bert/toxic_comment\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 159,572\n",
      "\n",
      "Number of training sentences: 19,947\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfli/anaconda3/envs/torch/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3051: DtypeWarning: Columns (2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/jigsaw-toxic-comment-classification-challenge/train.csv/train.csv',\n",
    "                header=None,\n",
    "                names=[\"id\",\"comment_text\",\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "                )\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "df.sample(10)\n",
    "\n",
    "df = df.loc[:df.shape[0]//8]\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(19947,)\n",
      "comment_text\n"
     ]
    }
   ],
   "source": [
    "sentences = df.comment_text.values\n",
    "print(type(sentences))\n",
    "print(sentences.shape)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"toxic\"] = pd.to_numeric(df[\"toxic\"], errors='coerce')\n",
    "df[\"severe_toxic\"] = pd.to_numeric(df[\"severe_toxic\"], errors='coerce')\n",
    "df[\"obscene\"] = pd.to_numeric(df[\"obscene\"], errors='coerce')\n",
    "df[\"threat\"] = pd.to_numeric(df[\"threat\"], errors='coerce')\n",
    "df[\"insult\"] = pd.to_numeric(df[\"insult\"], errors='coerce')\n",
    "df[\"identity_hate\"] = pd.to_numeric(df[\"identity_hate\"], errors='coerce')\n",
    "\n",
    "df['labels'] = df.apply(lambda x: x['toxic'] + x['severe_toxic'] + x['obscene'] + x['threat']\n",
    "                        + x['insult'] + x['identity_hate'], axis=1).map(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19947,) (19947,)\n",
      "[0 1]\n",
      "0    17892\n",
      "1     2055\n",
      "Name: labels, dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sentences.shape, labels.shape)\n",
    "print(df['labels'].unique())\n",
    "print(df['labels'].value_counts())\n",
    "labels = df.labels.values\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id                                       comment_text  \\\n",
      "16881  2c8589ec2367498b  A bit better, I imagine.  Almost went minimali...   \n",
      "12552  2140f889b3b2cf4c  \"\\n\\nFrom a purely WP:LEAD point of view, the ...   \n",
      "3254   08c82465cda26c7c  This is ridiculous. The user I'm speaking to a...   \n",
      "5421   0e713f2b7700ad46  Sorry, your protest goes unnoticed. For you to...   \n",
      "5542   0ec8f7a79be39166  Regarding edits made during April 13 2007 (UTC...   \n",
      "\n",
      "       toxic  severe_toxic  obscene  threat  insult  identity_hate  labels  \n",
      "16881    0.0           0.0      0.0     0.0     0.0            0.0       0  \n",
      "12552    0.0           0.0      0.0     0.0     0.0            0.0       0  \n",
      "3254     0.0           0.0      0.0     0.0     0.0            0.0       0  \n",
      "5421     0.0           0.0      0.0     0.0     0.0            0.0       0  \n",
      "5542     0.0           0.0      0.0     0.0     0.0            0.0       0  \n"
     ]
    }
   ],
   "source": [
    "# print(df.iloc[:1])\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  comment_text\n",
      "Tokenized:  ['comment', '_', 'text']\n",
      "Token IDs:  [7615, 1035, 3793]\n",
      "Original:  Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "Tokenized:  ['explanation', 'why', 'the', 'edit', '##s', 'made', 'under', 'my', 'user', '##name', 'hardcore', 'metallic', '##a', 'fan', 'were', 'reverted', '?', 'they', 'weren', \"'\", 't', 'van', '##dal', '##isms', ',', 'just', 'closure', 'on', 'some', 'gas', 'after', 'i', 'voted', 'at', 'new', 'york', 'dolls', 'fa', '##c', '.', 'and', 'please', 'don', \"'\", 't', 'remove', 'the', 'template', 'from', 'the', 'talk', 'page', 'since', 'i', \"'\", 'm', 'retired', 'now', '.', '89', '.', '205', '.', '38', '.', '27']\n",
      "Token IDs:  [7526, 2339, 1996, 10086, 2015, 2081, 2104, 2026, 5310, 18442, 13076, 12392, 2050, 5470, 2020, 16407, 1029, 2027, 4694, 1005, 1056, 3158, 9305, 22556, 1010, 2074, 8503, 2006, 2070, 3806, 2044, 1045, 5444, 2012, 2047, 2259, 14421, 6904, 2278, 1012, 1998, 3531, 2123, 1005, 1056, 6366, 1996, 23561, 2013, 1996, 2831, 3931, 2144, 1045, 1005, 1049, 3394, 2085, 1012, 6486, 1012, 16327, 1012, 4229, 1012, 2676]\n"
     ]
    }
   ],
   "source": [
    "print('Original: ', sentences[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\n",
    "\n",
    "# Don't know why it prints the attribution name\n",
    "print('Original: ', sentences[1])\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[1]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "Token IDs:  [101, 7526, 2339, 1996, 10086, 2015, 2081, 2104, 2026, 5310, 18442, 13076, 12392, 2050, 5470, 2020, 16407, 1029, 2027, 4694, 1005, 1056, 3158, 9305, 22556, 1010, 2074, 8503, 2006, 2070, 3806, 2044, 1045, 5444, 2012, 2047, 2259, 14421, 6904, 2278, 1012, 1998, 3531, 2123, 1005, 1056, 6366, 1996, 23561, 2013, 1996, 2831, 3931, 2144, 1045, 1005, 1049, 3394, 2085, 1012, 6486, 1012, 16327, 1012, 4229, 1012, 2676, 102]\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "for sent in sentences[1:]:\n",
    "    encoded_sent = tokenizer.encode(\n",
    "        sent,\n",
    "#         max_length=512,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "\n",
    "    )\n",
    "    input_ids.append(encoded_sent)\n",
    "print('Original: ', sentences[1])\n",
    "print('Token IDs: ', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_LEN = 512\n",
    "input_ids = pad_sequences(\n",
    "    input_ids,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='long',\n",
    "    value=0,\n",
    "    truncating='post',\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    input_ids,\n",
    "    labels[1:],\n",
    "    random_state=2020,\n",
    "    test_size=0.1\n",
    ")\n",
    "train_masks, validation_masks, _, _ = train_test_split(\n",
    "    attention_masks,\n",
    "    labels[1:],\n",
    "    random_state=2020,\n",
    "    test_size=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check trainset shape:  torch.Size([17951, 512]) torch.Size([17951])\n",
      "check validation set shape:  torch.Size([1995, 512]) torch.Size([1995])\n",
      "check masks shape:  17951 1995\n"
     ]
    }
   ],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "print(\"check trainset shape: \", train_inputs.shape, train_labels.shape)\n",
    "print(\"check validation set shape: \", validation_inputs.shape, validation_labels.shape)\n",
    "print(\"check masks shape: \", len(train_masks), len(validation_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 8\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/sfli/anaconda3/envs/torch/lib/python3.7/site-packages/torch/cuda/__init__.py:125: UserWarning: \n",
      "GeForce RTX 2080 Ti with CUDA capability sm_75 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the GeForce RTX 2080 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bert model has 201 different named parameters. \n",
      "\n",
      "====Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "print('The Bert model has {:} different named parameters. \\n'.format(len(params)))\n",
    "\n",
    "print('====Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('==== First Transformer ====\\n') \n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('==== Output Layer ====\\n') \n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 2e-5,\n",
    "    eps = 1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat==labels_flat) / len(labels_flat)\n",
    "\n",
    "def flat_auc(labels, preds):\n",
    "#     pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    pred_flat = preds[:, 1:].flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    fpr, tpr, thresholds = roc_curve(labels_flat, pred_flat, pos_label=2)\n",
    "    print()\n",
    "    return roc_auc_score(labels_flat, pred_flat)\n",
    "\n",
    "# true_labels = [0, 1, 0]\n",
    "# pre_labels = [0, 0, 0]\n",
    "# roc_auc_score(true_labels, pre_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Epoch 1 / 4 =======\n",
      "Batch    40 of 2,244.  Elapsed: 0:00:12.\n",
      "Batch    80 of 2,244.  Elapsed: 0:00:25.\n",
      "Batch   120 of 2,244.  Elapsed: 0:00:37.\n",
      "Batch   160 of 2,244.  Elapsed: 0:00:50.\n",
      "Batch   200 of 2,244.  Elapsed: 0:01:02.\n",
      "Batch   240 of 2,244.  Elapsed: 0:01:15.\n",
      "Batch   280 of 2,244.  Elapsed: 0:01:27.\n",
      "Batch   320 of 2,244.  Elapsed: 0:01:40.\n",
      "Batch   360 of 2,244.  Elapsed: 0:01:52.\n",
      "Batch   400 of 2,244.  Elapsed: 0:02:05.\n",
      "Batch   440 of 2,244.  Elapsed: 0:02:18.\n",
      "Batch   480 of 2,244.  Elapsed: 0:02:30.\n",
      "Batch   520 of 2,244.  Elapsed: 0:02:43.\n",
      "Batch   560 of 2,244.  Elapsed: 0:02:55.\n",
      "Batch   600 of 2,244.  Elapsed: 0:03:08.\n",
      "Batch   640 of 2,244.  Elapsed: 0:03:20.\n",
      "Batch   680 of 2,244.  Elapsed: 0:03:33.\n",
      "Batch   720 of 2,244.  Elapsed: 0:03:45.\n",
      "Batch   760 of 2,244.  Elapsed: 0:03:58.\n",
      "Batch   800 of 2,244.  Elapsed: 0:04:10.\n",
      "Batch   840 of 2,244.  Elapsed: 0:04:23.\n",
      "Batch   880 of 2,244.  Elapsed: 0:04:36.\n",
      "Batch   920 of 2,244.  Elapsed: 0:04:48.\n",
      "Batch   960 of 2,244.  Elapsed: 0:05:01.\n",
      "Batch 1,000 of 2,244.  Elapsed: 0:05:13.\n",
      "Batch 1,040 of 2,244.  Elapsed: 0:05:26.\n",
      "Batch 1,080 of 2,244.  Elapsed: 0:05:38.\n",
      "Batch 1,120 of 2,244.  Elapsed: 0:05:51.\n",
      "Batch 1,160 of 2,244.  Elapsed: 0:06:03.\n",
      "Batch 1,200 of 2,244.  Elapsed: 0:06:16.\n",
      "Batch 1,240 of 2,244.  Elapsed: 0:06:29.\n",
      "Batch 1,280 of 2,244.  Elapsed: 0:06:41.\n",
      "Batch 1,320 of 2,244.  Elapsed: 0:06:54.\n",
      "Batch 1,360 of 2,244.  Elapsed: 0:07:06.\n",
      "Batch 1,400 of 2,244.  Elapsed: 0:07:19.\n",
      "Batch 1,440 of 2,244.  Elapsed: 0:07:31.\n",
      "Batch 1,480 of 2,244.  Elapsed: 0:07:44.\n",
      "Batch 1,520 of 2,244.  Elapsed: 0:07:57.\n",
      "Batch 1,560 of 2,244.  Elapsed: 0:08:09.\n",
      "Batch 1,600 of 2,244.  Elapsed: 0:08:22.\n",
      "Batch 1,640 of 2,244.  Elapsed: 0:08:34.\n",
      "Batch 1,680 of 2,244.  Elapsed: 0:08:47.\n",
      "Batch 1,720 of 2,244.  Elapsed: 0:09:00.\n",
      "Batch 1,760 of 2,244.  Elapsed: 0:09:12.\n",
      "Batch 1,800 of 2,244.  Elapsed: 0:09:25.\n",
      "Batch 1,840 of 2,244.  Elapsed: 0:09:37.\n",
      "Batch 1,880 of 2,244.  Elapsed: 0:09:50.\n",
      "Batch 1,920 of 2,244.  Elapsed: 0:10:03.\n",
      "Batch 1,960 of 2,244.  Elapsed: 0:10:15.\n",
      "Batch 2,000 of 2,244.  Elapsed: 0:10:28.\n",
      "Batch 2,040 of 2,244.  Elapsed: 0:10:40.\n",
      "Batch 2,080 of 2,244.  Elapsed: 0:10:53.\n",
      "Batch 2,120 of 2,244.  Elapsed: 0:11:06.\n",
      "Batch 2,160 of 2,244.  Elapsed: 0:11:18.\n",
      "Batch 2,200 of 2,244.  Elapsed: 0:11:31.\n",
      "Batch 2,240 of 2,244.  Elapsed: 0:11:43.\n",
      "\n",
      " Average training loss: 0.01\n",
      " Training epoch took: 0:11:45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfli/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:788: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC score: 0.98\n",
      " Validation took: 0:00:26\n",
      "\n",
      "======= Epoch 2 / 4 =======\n",
      "Batch    40 of 2,244.  Elapsed: 0:00:13.\n",
      "Batch    80 of 2,244.  Elapsed: 0:00:25.\n",
      "Batch   120 of 2,244.  Elapsed: 0:00:38.\n",
      "Batch   160 of 2,244.  Elapsed: 0:00:51.\n",
      "Batch   200 of 2,244.  Elapsed: 0:01:03.\n",
      "Batch   240 of 2,244.  Elapsed: 0:01:16.\n",
      "Batch   280 of 2,244.  Elapsed: 0:01:28.\n",
      "Batch   320 of 2,244.  Elapsed: 0:01:41.\n",
      "Batch   360 of 2,244.  Elapsed: 0:01:54.\n",
      "Batch   400 of 2,244.  Elapsed: 0:02:06.\n",
      "Batch   440 of 2,244.  Elapsed: 0:02:19.\n",
      "Batch   480 of 2,244.  Elapsed: 0:02:32.\n",
      "Batch   520 of 2,244.  Elapsed: 0:02:44.\n",
      "Batch   560 of 2,244.  Elapsed: 0:02:57.\n",
      "Batch   600 of 2,244.  Elapsed: 0:03:10.\n",
      "Batch   640 of 2,244.  Elapsed: 0:03:22.\n",
      "Batch   680 of 2,244.  Elapsed: 0:03:35.\n",
      "Batch   720 of 2,244.  Elapsed: 0:03:48.\n",
      "Batch   760 of 2,244.  Elapsed: 0:04:00.\n",
      "Batch   800 of 2,244.  Elapsed: 0:04:13.\n",
      "Batch   840 of 2,244.  Elapsed: 0:04:26.\n",
      "Batch   880 of 2,244.  Elapsed: 0:04:38.\n",
      "Batch   920 of 2,244.  Elapsed: 0:04:51.\n",
      "Batch   960 of 2,244.  Elapsed: 0:05:04.\n",
      "Batch 1,000 of 2,244.  Elapsed: 0:05:16.\n",
      "Batch 1,040 of 2,244.  Elapsed: 0:05:29.\n",
      "Batch 1,080 of 2,244.  Elapsed: 0:05:42.\n",
      "Batch 1,120 of 2,244.  Elapsed: 0:05:54.\n",
      "Batch 1,160 of 2,244.  Elapsed: 0:06:07.\n",
      "Batch 1,200 of 2,244.  Elapsed: 0:06:19.\n",
      "Batch 1,240 of 2,244.  Elapsed: 0:06:32.\n",
      "Batch 1,280 of 2,244.  Elapsed: 0:06:45.\n",
      "Batch 1,320 of 2,244.  Elapsed: 0:06:57.\n",
      "Batch 1,360 of 2,244.  Elapsed: 0:07:10.\n",
      "Batch 1,400 of 2,244.  Elapsed: 0:07:23.\n",
      "Batch 1,440 of 2,244.  Elapsed: 0:07:35.\n",
      "Batch 1,480 of 2,244.  Elapsed: 0:07:48.\n",
      "Batch 1,520 of 2,244.  Elapsed: 0:08:01.\n",
      "Batch 1,560 of 2,244.  Elapsed: 0:08:13.\n",
      "Batch 1,600 of 2,244.  Elapsed: 0:08:26.\n",
      "Batch 1,640 of 2,244.  Elapsed: 0:08:39.\n",
      "Batch 1,680 of 2,244.  Elapsed: 0:08:51.\n",
      "Batch 1,720 of 2,244.  Elapsed: 0:09:04.\n",
      "Batch 1,760 of 2,244.  Elapsed: 0:09:17.\n",
      "Batch 1,800 of 2,244.  Elapsed: 0:09:29.\n",
      "Batch 1,840 of 2,244.  Elapsed: 0:09:42.\n",
      "Batch 1,880 of 2,244.  Elapsed: 0:09:55.\n",
      "Batch 1,920 of 2,244.  Elapsed: 0:10:07.\n",
      "Batch 1,960 of 2,244.  Elapsed: 0:10:20.\n",
      "Batch 2,000 of 2,244.  Elapsed: 0:10:33.\n",
      "Batch 2,040 of 2,244.  Elapsed: 0:10:45.\n",
      "Batch 2,080 of 2,244.  Elapsed: 0:10:58.\n",
      "Batch 2,120 of 2,244.  Elapsed: 0:11:10.\n",
      "Batch 2,160 of 2,244.  Elapsed: 0:11:23.\n",
      "Batch 2,200 of 2,244.  Elapsed: 0:11:36.\n",
      "Batch 2,240 of 2,244.  Elapsed: 0:11:48.\n",
      "\n",
      " Average training loss: 0.02\n",
      " Training epoch took: 0:11:50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfli/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:788: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC score: 0.98\n",
      " Validation took: 0:00:26\n",
      "\n",
      "======= Epoch 3 / 4 =======\n",
      "Batch    40 of 2,244.  Elapsed: 0:00:13.\n",
      "Batch    80 of 2,244.  Elapsed: 0:00:25.\n",
      "Batch   120 of 2,244.  Elapsed: 0:00:38.\n",
      "Batch   160 of 2,244.  Elapsed: 0:00:51.\n",
      "Batch   200 of 2,244.  Elapsed: 0:01:03.\n",
      "Batch   240 of 2,244.  Elapsed: 0:01:16.\n",
      "Batch   280 of 2,244.  Elapsed: 0:01:29.\n",
      "Batch   320 of 2,244.  Elapsed: 0:01:41.\n",
      "Batch   360 of 2,244.  Elapsed: 0:01:54.\n",
      "Batch   400 of 2,244.  Elapsed: 0:02:07.\n",
      "Batch   440 of 2,244.  Elapsed: 0:02:19.\n",
      "Batch   480 of 2,244.  Elapsed: 0:02:32.\n",
      "Batch   520 of 2,244.  Elapsed: 0:02:45.\n",
      "Batch   560 of 2,244.  Elapsed: 0:02:57.\n",
      "Batch   600 of 2,244.  Elapsed: 0:03:10.\n",
      "Batch   640 of 2,244.  Elapsed: 0:03:23.\n",
      "Batch   680 of 2,244.  Elapsed: 0:03:35.\n",
      "Batch   720 of 2,244.  Elapsed: 0:03:48.\n",
      "Batch   760 of 2,244.  Elapsed: 0:04:01.\n",
      "Batch   800 of 2,244.  Elapsed: 0:04:13.\n",
      "Batch   840 of 2,244.  Elapsed: 0:04:26.\n",
      "Batch   880 of 2,244.  Elapsed: 0:04:39.\n",
      "Batch   920 of 2,244.  Elapsed: 0:04:51.\n",
      "Batch   960 of 2,244.  Elapsed: 0:05:04.\n",
      "Batch 1,000 of 2,244.  Elapsed: 0:05:16.\n",
      "Batch 1,040 of 2,244.  Elapsed: 0:05:29.\n",
      "Batch 1,080 of 2,244.  Elapsed: 0:05:42.\n",
      "Batch 1,120 of 2,244.  Elapsed: 0:05:54.\n",
      "Batch 1,160 of 2,244.  Elapsed: 0:06:07.\n",
      "Batch 1,200 of 2,244.  Elapsed: 0:06:20.\n",
      "Batch 1,240 of 2,244.  Elapsed: 0:06:32.\n",
      "Batch 1,280 of 2,244.  Elapsed: 0:06:45.\n",
      "Batch 1,320 of 2,244.  Elapsed: 0:06:58.\n",
      "Batch 1,360 of 2,244.  Elapsed: 0:07:10.\n",
      "Batch 1,400 of 2,244.  Elapsed: 0:07:23.\n",
      "Batch 1,440 of 2,244.  Elapsed: 0:07:36.\n",
      "Batch 1,480 of 2,244.  Elapsed: 0:07:48.\n",
      "Batch 1,520 of 2,244.  Elapsed: 0:08:01.\n",
      "Batch 1,560 of 2,244.  Elapsed: 0:08:14.\n",
      "Batch 1,600 of 2,244.  Elapsed: 0:08:26.\n",
      "Batch 1,640 of 2,244.  Elapsed: 0:08:39.\n",
      "Batch 1,680 of 2,244.  Elapsed: 0:08:52.\n",
      "Batch 1,720 of 2,244.  Elapsed: 0:09:04.\n",
      "Batch 1,760 of 2,244.  Elapsed: 0:09:17.\n",
      "Batch 1,800 of 2,244.  Elapsed: 0:09:30.\n",
      "Batch 1,840 of 2,244.  Elapsed: 0:09:42.\n",
      "Batch 1,880 of 2,244.  Elapsed: 0:09:55.\n",
      "Batch 1,920 of 2,244.  Elapsed: 0:10:08.\n",
      "Batch 1,960 of 2,244.  Elapsed: 0:10:20.\n",
      "Batch 2,000 of 2,244.  Elapsed: 0:10:33.\n",
      "Batch 2,040 of 2,244.  Elapsed: 0:10:46.\n",
      "Batch 2,080 of 2,244.  Elapsed: 0:10:59.\n",
      "Batch 2,120 of 2,244.  Elapsed: 0:11:11.\n",
      "Batch 2,160 of 2,244.  Elapsed: 0:11:24.\n",
      "Batch 2,200 of 2,244.  Elapsed: 0:11:37.\n",
      "Batch 2,240 of 2,244.  Elapsed: 0:11:49.\n",
      "\n",
      " Average training loss: 0.01\n",
      " Training epoch took: 0:11:51\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfli/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:788: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC score: 0.98\n",
      " Validation took: 0:00:26\n",
      "\n",
      "======= Epoch 4 / 4 =======\n",
      "Batch    40 of 2,244.  Elapsed: 0:00:13.\n",
      "Batch    80 of 2,244.  Elapsed: 0:00:25.\n",
      "Batch   120 of 2,244.  Elapsed: 0:00:38.\n",
      "Batch   160 of 2,244.  Elapsed: 0:00:51.\n",
      "Batch   200 of 2,244.  Elapsed: 0:01:03.\n",
      "Batch   240 of 2,244.  Elapsed: 0:01:16.\n",
      "Batch   280 of 2,244.  Elapsed: 0:01:29.\n",
      "Batch   320 of 2,244.  Elapsed: 0:01:41.\n",
      "Batch   360 of 2,244.  Elapsed: 0:01:54.\n",
      "Batch   400 of 2,244.  Elapsed: 0:02:07.\n",
      "Batch   440 of 2,244.  Elapsed: 0:02:20.\n",
      "Batch   480 of 2,244.  Elapsed: 0:02:32.\n",
      "Batch   520 of 2,244.  Elapsed: 0:02:45.\n",
      "Batch   560 of 2,244.  Elapsed: 0:02:58.\n",
      "Batch   600 of 2,244.  Elapsed: 0:03:10.\n",
      "Batch   640 of 2,244.  Elapsed: 0:03:23.\n",
      "Batch   680 of 2,244.  Elapsed: 0:03:36.\n",
      "Batch   720 of 2,244.  Elapsed: 0:03:48.\n",
      "Batch   760 of 2,244.  Elapsed: 0:04:01.\n",
      "Batch   800 of 2,244.  Elapsed: 0:04:14.\n",
      "Batch   840 of 2,244.  Elapsed: 0:04:26.\n",
      "Batch   880 of 2,244.  Elapsed: 0:04:39.\n",
      "Batch   920 of 2,244.  Elapsed: 0:04:52.\n",
      "Batch   960 of 2,244.  Elapsed: 0:05:04.\n",
      "Batch 1,000 of 2,244.  Elapsed: 0:05:17.\n",
      "Batch 1,040 of 2,244.  Elapsed: 0:05:30.\n",
      "Batch 1,080 of 2,244.  Elapsed: 0:05:42.\n",
      "Batch 1,120 of 2,244.  Elapsed: 0:05:55.\n",
      "Batch 1,160 of 2,244.  Elapsed: 0:06:08.\n",
      "Batch 1,200 of 2,244.  Elapsed: 0:06:20.\n",
      "Batch 1,240 of 2,244.  Elapsed: 0:06:33.\n",
      "Batch 1,280 of 2,244.  Elapsed: 0:06:46.\n",
      "Batch 1,320 of 2,244.  Elapsed: 0:06:58.\n",
      "Batch 1,360 of 2,244.  Elapsed: 0:07:11.\n",
      "Batch 1,400 of 2,244.  Elapsed: 0:07:24.\n",
      "Batch 1,440 of 2,244.  Elapsed: 0:07:37.\n",
      "Batch 1,480 of 2,244.  Elapsed: 0:07:49.\n",
      "Batch 1,520 of 2,244.  Elapsed: 0:08:02.\n",
      "Batch 1,560 of 2,244.  Elapsed: 0:08:15.\n",
      "Batch 1,600 of 2,244.  Elapsed: 0:08:27.\n",
      "Batch 1,640 of 2,244.  Elapsed: 0:08:40.\n",
      "Batch 1,680 of 2,244.  Elapsed: 0:08:53.\n",
      "Batch 1,720 of 2,244.  Elapsed: 0:09:05.\n",
      "Batch 1,760 of 2,244.  Elapsed: 0:09:18.\n",
      "Batch 1,800 of 2,244.  Elapsed: 0:09:31.\n",
      "Batch 1,840 of 2,244.  Elapsed: 0:09:43.\n",
      "Batch 1,880 of 2,244.  Elapsed: 0:09:56.\n",
      "Batch 1,920 of 2,244.  Elapsed: 0:10:09.\n",
      "Batch 1,960 of 2,244.  Elapsed: 0:10:21.\n",
      "Batch 2,000 of 2,244.  Elapsed: 0:10:34.\n",
      "Batch 2,040 of 2,244.  Elapsed: 0:10:47.\n",
      "Batch 2,080 of 2,244.  Elapsed: 0:11:00.\n",
      "Batch 2,120 of 2,244.  Elapsed: 0:11:12.\n",
      "Batch 2,160 of 2,244.  Elapsed: 0:11:25.\n",
      "Batch 2,200 of 2,244.  Elapsed: 0:11:38.\n",
      "Batch 2,240 of 2,244.  Elapsed: 0:11:50.\n",
      "\n",
      " Average training loss: 0.01\n",
      " Training epoch took: 0:11:52\n",
      "\n",
      "\n",
      "AUC score: 0.98\n",
      " Validation took: 0:00:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfli/anaconda3/envs/torch/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:788: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "# random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "loss_values = []\n",
    "for epoch_i in range(epochs):\n",
    "    print(\"\")\n",
    "    print(\"======= Epoch {:} / {:} =======\".format(epoch_i+1, epochs))\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('Batch {:>5,} of {:>5,}.  Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_ids = b_input_ids.to(torch.int64)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\" Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\" Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    print(\"\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "#     eval_loss, eval_accuracy = 0, 0\n",
    "#     nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    true_arr, pred_arr = [], []\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids = b_input_ids.to(torch.int64)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask\n",
    "            )\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "#         print(logits.shape, label_ids.shape) # (8, 2) (8,)\n",
    "        true_arr.append(label_ids)\n",
    "        pred_arr.append(logits)\n",
    "#         tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "#         eval_accuracy += tmp_eval_accuracy\n",
    "#         nb_eval_steps += 1\n",
    "    true_arr = np.concatenate(true_arr, axis=0)\n",
    "    pred_arr = np.concatenate(pred_arr, axis=0)\n",
    "    auc_score = flat_auc(true_arr, pred_arr)\n",
    "#     print(\" Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"AUC score: {0:.2f}\".format(auc_score))\n",
    "    print(\" Validation took: {:}\".format(format_time(time.time() - t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "true_arr, pred_arr = [], []\n",
    "logits_1 = np.ones([3,2])\n",
    "logits_2 = np.ones([3,2])\n",
    "true_arr.append(logits_1)\n",
    "true_arr.append(logits_2)\n",
    "# true_label_1 = np.array([1])\n",
    "# print(true_label_1.shape)\n",
    "# true_label_2 = np.array([0])\n",
    "# pred_arr.append(true_label_1)\n",
    "# pred_arr.append(true_label_2)\n",
    "# pred_arr = np.concatenate(pred_arr, axis=0)\n",
    "# print(pred_arr, pred_arr.shape)\n",
    "\n",
    "# pred_arr_2 = np.array([0, 1])\n",
    "# print(pred_arr_2)\n",
    "\n",
    "# pred_arr = np.concatenate([pred_arr, pred_arr_2], axis=0)\n",
    "# print(pred_arr, pred_arr.shape)\n",
    "true_arr = np.concatenate(true_arr, axis=0)\n",
    "\n",
    "# print(logits_1.shape)\n",
    "print(true_arr[:, 1:].flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
